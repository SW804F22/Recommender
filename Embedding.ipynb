{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aebedcd1-3b4f-4511-80c5-38fc632073a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import Normalizer , scale\n",
    "from sklearn.model_selection import GridSearchCV , KFold , cross_val_score\n",
    "from sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error \n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5c2acb48-8b91-453b-a0e1-28b59bbc6703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4124fa28-fbb1-4246-9c94-37950628ce9e",
   "metadata": {},
   "source": [
    "Loading csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07288e26-a08b-47c2-bda2-dd9f52d52566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_TIST2015_Checkins: User ID, Venue ID, UTC time, Time zone\n",
    "#dataset_TIST2015_POIs: Venue ID, Latitude, Longitude, Venue category name, Country code \n",
    "#dataset_TIST2015_Cities: Cityname, Latitude, Longitude, Vanue Category, Country code, Country name, City type\n",
    "#dataset_UbiComp2016_UserProfile_{CITY}: User ID, Gender, Twitter friend count, Twitter follower count\n",
    "\n",
    "#Scenario 1: Checkins og POI locations\n",
    "\n",
    "checkin_cols = ['user_id', 'poi_id', 'timestamp', 'timezone']\n",
    "checkin = pd.read_csv(r'C:\\Users\\lasse\\Desktop\\RecommenderDL\\datasets\\TIST2015_Checkins.csv', sep=',', names=checkin_cols, encoding='latin-1')\n",
    "\n",
    "city_cols = ['city_name', 'citycenter_latitude', 'citycenter_longitude', 'country_code', 'country_name', 'city_type']\n",
    "cities = pd.read_csv(r'C:\\Users\\lasse\\Desktop\\RecommenderDL\\datasets\\TIST2015_Cities.csv', sep=',', names=city_cols, encoding='latin-1')\n",
    "\n",
    "venue_cols = ['poi_id', 'latitude', 'longitude', 'category', 'country_code']\n",
    "pois = pd.read_csv(r'C:\\Users\\lasse\\Desktop\\RecommenderDL\\datasets\\TIST2015_POIs.csv', sep=',', names=venue_cols, encoding='latin-1')\n",
    "\n",
    "\n",
    "#Scenario 2: Users og Checkins\n",
    "\n",
    "# Load each data set (users, movies, and ratings).\n",
    "users_cols = ['user_id', 'gender', 'sex']\n",
    "ratings_cols = ['user_id', 'venue_id', 'latitude', 'longitude', 'category' 'unix_timestamp']\n",
    "\n",
    "#Scenario3: Users, Checkins og POI Locations\n",
    "checkin_cols = ['poi_id', 'gender', 'sex']\n",
    "venue_cols = ['venue_id', 'user_id', 'latitude', 'longitude', 'category' 'unix_timestamp']\n",
    "user_cols = ['user_id', 'gender']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575757c8-4557-4929-86e5-17bcedda1d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim1 = cities[['city_name', 'citycenter_longitude', 'citycenter_latitude', 'city_type', 'country_code']].drop(labels=[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbb8c22-bad2-4880-bfad-ba88e32a66a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim2 = checkin[['user_id', 'poi_id', 'timestamp']].drop(labels=[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311e3032-b813-4e94-8747-f3a0ebd056a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim3 = pois[['poi_id', 'latitude', 'longitude', 'category', 'country_code']].drop(labels=[0], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ff05e9-4946-42ff-8d25-b3a1420fb172",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dim2.merge(dim3, on='poi_id')#.merge(dim1, on='country_code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7bf439-7166-4587-b130-55581b643c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Users and Checkins. \n",
    "# However, our data does not allow us to merge users and checkin, because we only have data for users that have checked in NYC and Tokyo\n",
    "#movielens = ratings.merge(movies, on='movie_id').merge(users, on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c203e99-1b95-473f-b48a-7d9d262f4072",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into train and test\n",
    "train_, test = train_test_split(dataset, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a35fd34-2082-491b-8ac4-e772586adc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROW_COUNT = train_.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc86265-e948-434b-a18b-55cd67f14309",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 10\n",
    "NUM_USERS = dataset['user_id'].nunique()\n",
    "NUM_POI = dataset['poi_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea98359-9bee-4bda-aca6-a723796d9d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aab5722-fcad-4031-bf76-70e135783632",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIQUE_MOVIE_IDS = dataset['movie_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c513afd-13ea-45d2-8d14-a7542907b544",
   "metadata": {},
   "source": [
    "# Create Subset of Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e7b912-a16b-4fce-8cd1-aeddfc256bd3",
   "metadata": {},
   "source": [
    "## Load in the dataset from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f60a8ef0-abfd-496b-b2c8-c391a8893b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lasse\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3437: DtypeWarning: Columns (0,3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\lasse\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3437: DtypeWarning: Columns (1,2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "checkin_cols = ['user_id', 'poi_id', 'timestamp', 'timezone']\n",
    "checkins = pd.read_csv(r'C:\\Users\\lasse\\Desktop\\RecommenderDL\\datasets\\TIST2015_Checkins.csv', sep=',', names=checkin_cols, encoding='latin-1')\n",
    "\n",
    "venue_cols = ['poi_id', 'latitude', 'longitude', 'category', 'country_code']\n",
    "pois = pd.read_csv(r'C:\\Users\\lasse\\Desktop\\RecommenderDL\\datasets\\TIST2015_POIs.csv', sep=',', names=venue_cols, encoding='latin-1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eb582d-b794-4092-951d-65811c0dc0e8",
   "metadata": {},
   "source": [
    "## Function that extracts a subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81307241-332e-4373-adbc-23c62bfd90f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subset of a dataset with size n users\n",
    "def create_checkin_subset(checkins_table, n):\n",
    "    subset = np.array([[0,0,0,0],[0,0,0,0]])\n",
    "    checkins_100_users = checkins_table['user_id'].sample(n=n, random_state=1)\n",
    "    for i in checkins_100_users:\n",
    "        subset = np.append(subset, np.asarray(checkins_table[checkins_table['user_id'] == i].values), 0)\n",
    "    return pd.DataFrame(subset[2::], columns=['user_id', 'poi_id', 'timestamp', 'timezone'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33354896-3b67-4385-ac77-e78e4055fc51",
   "metadata": {},
   "source": [
    "### Calling the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d01f1bcd-57f4-4ef6-beb5-747a4778bde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the subset method\n",
    "checkins_100_users = create_checkin_subset(checkins, 25)\n",
    "checkin_data = checkins_100_users.merge(pois, on='poi_id')\n",
    "df = checkin_data.set_index('user_id').poi_id.str.get_dummies(',')\n",
    "df = df.groupby('user_id').max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b02856a-4a4c-47ea-a41b-ce946dc39f40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e98ae34-3640-4281-b8f5-7a3e188199d9",
   "metadata": {},
   "source": [
    "## Extracting all of the users and the pois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f456903-18e7-4ee4-8490-ca4f08cbcbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "listofusers = pd.DataFrame(checkin_data, columns= ['user_id']).groupby('user_id').max().sample(frac=1)\n",
    "listofpois = pd.DataFrame(checkin_data, columns= ['poi_id', 'latitude', 'longitude']).groupby('poi_id').max().sample(frac=1)\n",
    "\n",
    "userarray = listofusers.index.to_numpy()\n",
    "poiarray = listofpois.index.to_numpy()\n",
    "\n",
    "userdataframe = pd.DataFrame(userarray, columns = ['Users'])\n",
    "poidataframe = pd.DataFrame(poiarray, columns = ['Poi'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfba402-48b1-4486-a1c7-91b710b9382c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REDUNDANT, i think\n",
    "\n",
    "rows_list = []\n",
    "for i in range(len(poidataframe)):\n",
    "    longitude = listofpois[listofpois.loc[i, \"Poi\"]]\n",
    "    rows_list.append(dict1)\n",
    "\n",
    "latlong = pd.DataFrame(rows_list) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "850a4d5d-20e1-4a67-9ed2-f884749961cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot = userdataframe.merge(poidataframe, how='cross')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0ce37a-d574-450c-b78b-9c82d27f73d0",
   "metadata": {},
   "source": [
    "## Extracting ground_truth from incidence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9fc84c7-d6a4-4357-90b7-301f0d27482b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_list = []\n",
    "for i in range(len(dot)):\n",
    "    temp = df[dot.loc[i, \"Poi\"]][dot.loc[i, \"Users\"]]\n",
    "    dict1 = {'ground_truth':float(temp)}\n",
    "    rows_list.append(dict1)\n",
    "\n",
    "groundtruth = pd.DataFrame(rows_list) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2f25e77-7dac-41b3-81b8-996c125ecfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([dot, groundtruth], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d5c7a8-13e9-4281-bd5e-4874b4888bbf",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a978e995-e5ba-4b42-a003-6f07f6a98d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbModel(Model):\n",
    "    def __init__(self):\n",
    "        super(EmbModel, self).__init__()\n",
    "        self.model = self.init_model()\n",
    "        self.d_steps = 16\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return\n",
    "    \n",
    "    def init_model(self):\n",
    "        poi_latitude_input = keras.Input(shape=(1,), name='poi_latitude')\n",
    "        poi_longitude_input = keras.Input(shape=(1,), name='poi_longitude')\n",
    "        poi_concat_input = tf.keras.layers.Concatenate(axis=-1)([poi_latitude_input, poi_longitude_input])\n",
    "        #input_length:  #This is the length of input sequences, as you would define for any input layer of a Keras model. \n",
    "                        #For example, if all of your input documents are comprised of 1000 words, this would be 1000\n",
    "        #input_dim: \n",
    "                        #This is the size of the vocabulary in the text data. \n",
    "                        #For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
    "        poi_emb = layers.Dense(50)(poi_concat_input)\n",
    "        #poi_reshape = layers.Reshape((1,10))(poi_emb)\n",
    "    \n",
    "        user_input = keras.Input(shape=(1,), name='user_id')\n",
    "        #user_dense = keras.layers.Dense(2)(user_input)\n",
    "        user_emb = layers.Dense(50)(user_input)\n",
    "        \n",
    "                                    \n",
    "        dot = layers.Dot(axes=(1))([poi_emb, user_emb])\n",
    "        \n",
    "        \n",
    "        model = Model([[poi_latitude_input, poi_longitude_input], user_input], dot)\n",
    "        model.summary()\n",
    "        return model\n",
    "    \n",
    "    def compile_model(self, optimizer):\n",
    "        super(EmbModel, self).compile(run_eagerly=True)\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "    def train_step(self, data):\n",
    "        if len(data) == 3:\n",
    "            real_data, labels, sample_weight = data\n",
    "        else:\n",
    "            sample_weight = None\n",
    "            real_data, labels = data\n",
    "        user_data = real_data[0]\n",
    "        latlong_data = real_data[1]\n",
    "        \n",
    "        for i in range(self.d_steps):\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                \n",
    "                #print(latlong_data[0])\n",
    "                #print(latlong_data[1])\n",
    "                #print(user_data)\n",
    "                \n",
    "                dotproduct = self.model([[latlong_data[0], latlong_data[1]], user_data])\n",
    "                #print(dotproduct)\n",
    "                # Loss function = ||S-GroundTruth|| \n",
    "                loss = tf.math.abs(tf.subtract(tf.cast(dotproduct, tf.float64), labels))\n",
    "                #print(loss)\n",
    "            d_gradient = tape.gradient(loss, self.model.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(d_gradient, self.model.trainable_variables))\n",
    "        return {'loss': loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "950bec32-baf7-43b9-9ab1-b3443c46ab4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_31\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "poi_latitude (InputLayer)       [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "poi_longitude (InputLayer)      [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 2)            0           poi_latitude[0][0]               \n",
      "                                                                 poi_longitude[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "user_id (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 50)           150         concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 50)           100         user_id[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_16 (Dot)                    (None, 1)            0           dense_9[0][0]                    \n",
      "                                                                 dense_10[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 250\n",
      "Trainable params: 250\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = EmbModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "821d7533-e32d-4b02-aedf-d63d80e967d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           latitude   longitude\n",
      "poi_id                                         \n",
      "4b763f1df964a5200e452ee3  35.510588  139.631317\n",
      "4e176814b61c3f2c4879279a  13.726529  100.530999\n",
      "50852ad2e4b08785ea69a703   35.67134  139.702852\n",
      "4c73882bf3279c74a2dfb32d  35.439519  139.644728\n",
      "4b36fbbef964a520d33e25e3  35.711149  139.774455\n",
      "...                             ...         ...\n",
      "4bb3649635f0c9b68049bc83  18.768972   98.974961\n",
      "4d9c64e45c33a35dbccce8a0  51.518192   -0.116432\n",
      "4cfebb7dd7206ea8b5d64f69  34.665246  135.529924\n",
      "4b625594f964a520e4422ae3  42.341927   -71.05563\n",
      "51e06112498e02ca8d08debf  19.421879  -99.116526\n",
      "\n",
      "[2896 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(listofpois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8830ef42-101c-404c-805c-1d334220ded1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4b763f1df964a5200e452ee3']\n",
      "37.843275\n"
     ]
    }
   ],
   "source": [
    "print([poidataframe.loc[0, \"Poi\"]])\n",
    "print(listofpois.loc['4c576c0ccc96c9b6a70f792e']['latitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26deb392-8e1b-450e-985d-02b37cce35a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_list = []\n",
    "for i in range(len(dot)):\n",
    "    temp = dot.loc[i, \"Poi\"]\n",
    "    latitude = listofpois.loc[temp]['latitude']\n",
    "    longitude = listofpois.loc[temp]['longitude']\n",
    "    dict1 = {'latitude':latitude, 'longitude':longitude}\n",
    "    rows_list.append(dict1)\n",
    "    #latitude = poiarray[i]\n",
    "latlong = pd.DataFrame(rows_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5c4b1f-fed4-42d4-8a13-e50f41b2473c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76b97c63-6540-41be-8200-b7abf1a4b55d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "userdot = pd.DataFrame(dot, columns= ['Users'])\n",
    "latlong['latitude'] = pd.to_numeric(latlong['latitude'])\n",
    "latlong['longitude'] = pd.to_numeric(latlong['longitude'])\n",
    "dataset = pd.concat([userdot, latlong], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "763eceef-a5a8-41a6-afa0-acb08a33ffc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_numpy = dataset.to_numpy()\n",
    "labels_numpy = groundtruth.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f499510c-7a11-40be-b4f0-96f5df4263a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset_numpy[0][0]))\n",
    "print(type(dataset_numpy[0][1]))\n",
    "print(type(dataset_numpy[0][2]))\n",
    "print(type(labels_numpy[0][0]))\n",
    "#print(labels_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c4885c2-ba35-4ca6-95e2-c9dce72f2ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Users\n",
      "0        451\n",
      "1        451\n",
      "2        451\n",
      "3        451\n",
      "4        451\n",
      "...      ...\n",
      "72395  29150\n",
      "72396  29150\n",
      "72397  29150\n",
      "72398  29150\n",
      "72399  29150\n",
      "\n",
      "[72400 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(userdot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44dce8f5-e86c-4fd2-b5e2-9ccca00caee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset1 = tf.convert_to_tensor(\n",
    "    userdot, dtype=None, dtype_hint=None, name=None)\n",
    "dataset2 = tf.convert_to_tensor(\n",
    "    latlong, dtype=None, dtype_hint=None, name=None)\n",
    "labels = tf.convert_to_tensor(\n",
    "    labels_numpy, dtype=None, dtype_hint=None, name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "92e66365-d7a9-4552-b5e3-3419e7d0fea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 35.510588 139.631317]\n",
      " [ 13.726529 100.530999]\n",
      " [ 35.67134  139.702852]\n",
      " ...\n",
      " [ 34.665246 135.529924]\n",
      " [ 42.341927 -71.05563 ]\n",
      " [ 19.421879 -99.116526]], shape=(72400, 2), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "print(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8dc98d15-f5c8-48b7-a161-b0562c684ebb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "28953/28960 [============================>.] - ETA: 0s - loss: 3956.0721WARNING:tensorflow:Layer emb_model_19 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "28960/28960 [==============================] - 52s 2ms/step - loss: 3954.9794 - val_loss: 0.0000e+00\n",
      "Epoch 2/20\n",
      "28960/28960 [==============================] - 54s 2ms/step - loss: 105.7293 - val_loss: 0.0000e+00\n",
      "Epoch 3/20\n",
      "28960/28960 [==============================] - 54s 2ms/step - loss: 106.7121 - val_loss: 0.0000e+00\n",
      "Epoch 4/20\n",
      "28960/28960 [==============================] - 59s 2ms/step - loss: 103.4240 - val_loss: 0.0000e+00\n",
      "Epoch 5/20\n",
      "28960/28960 [==============================] - 58s 2ms/step - loss: 106.1105 - val_loss: 0.0000e+00\n",
      "Epoch 6/20\n",
      "28960/28960 [==============================] - 52s 2ms/step - loss: 106.9169 - val_loss: 0.0000e+00\n",
      "Epoch 7/20\n",
      "28960/28960 [==============================] - 59s 2ms/step - loss: 106.7144 - val_loss: 0.0000e+00\n",
      "Epoch 8/20\n",
      "28960/28960 [==============================] - 56s 2ms/step - loss: 105.8532 - val_loss: 0.0000e+00\n",
      "Epoch 9/20\n",
      "28960/28960 [==============================] - 56s 2ms/step - loss: 107.3012 - val_loss: 0.0000e+00\n",
      "Epoch 10/20\n",
      "28960/28960 [==============================] - 55s 2ms/step - loss: 105.1449 - val_loss: 0.0000e+00\n",
      "Epoch 11/20\n",
      "28960/28960 [==============================] - 56s 2ms/step - loss: 108.1779 - val_loss: 0.0000e+00\n",
      "Epoch 12/20\n",
      "28960/28960 [==============================] - 58s 2ms/step - loss: 111.1243 - val_loss: 0.0000e+00\n",
      "Epoch 13/20\n",
      "28960/28960 [==============================] - 56s 2ms/step - loss: 101.8735 - val_loss: 0.0000e+00\n",
      "Epoch 14/20\n",
      "28960/28960 [==============================] - 56s 2ms/step - loss: 105.1031 - val_loss: 0.0000e+00\n",
      "Epoch 15/20\n",
      "28960/28960 [==============================] - 56s 2ms/step - loss: 104.8886 - val_loss: 0.0000e+00\n",
      "Epoch 16/20\n",
      "28960/28960 [==============================] - 56s 2ms/step - loss: 109.4367 - val_loss: 0.0000e+00\n",
      "Epoch 17/20\n",
      "28960/28960 [==============================] - 58s 2ms/step - loss: 109.3593 - val_loss: 0.0000e+00\n",
      "Epoch 18/20\n",
      "28960/28960 [==============================] - 57s 2ms/step - loss: 107.1688 - val_loss: 0.0000e+00\n",
      "Epoch 19/20\n",
      "28960/28960 [==============================] - 57s 2ms/step - loss: 109.2625 - val_loss: 0.0000e+00\n",
      "Epoch 20/20\n",
      "28960/28960 [==============================] - 57s 2ms/step - loss: 106.5717 - val_loss: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f5817276a0>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001, beta_1=0.0, beta_2=0.99, epsilon=1e-8)\n",
    "\n",
    "model.compile(\n",
    "    optimizer\n",
    ")\n",
    "\n",
    "#Train_data, [dataset.user_id, dataset.poi_id]. Label: ground_truth\n",
    "model.fit([dataset1, dataset2], labels, epochs = 20, validation_split = 0.2, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd804c5d-2a2c-4964-960e-de7aa6054914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa856b6-8f5c-4af3-918c-ce2ca9f22448",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
